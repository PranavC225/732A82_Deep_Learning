{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8SC3p-UCusw"
   },
   "source": [
    "# RNN Machine Translation Laboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvKNkEYrCusy"
   },
   "source": [
    "In this lab, your task is to build a sequence-to-sequence model, using recurrent neural networks, that translates short sentences from Swedish into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Tensorflow is quite chatty; filter out warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IDNS-GhLCusz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 11:14:05.222281: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744449245.244090  491396 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744449245.250713  491396 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "from tf_keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that Tensorflow uses a GPU _(optional)_\n",
    "\n",
    "Training the models in this notebook can be sped up significantly with a GPU.  The following cell can be used to check if the GPU is set up correctly.  If you run on CPU, you can either run or just ignore this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úó Tensorflow has NOT detected a GPU.\n",
      "\n",
      "For GPU support, visit: <https://www.tensorflow.org/install/pip>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 11:14:20.566656: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    print(\"‚úì Tensorflow has detected a GPU.\")\n",
    "    import shutil\n",
    "    if not shutil.which(\"ptxas\"):\n",
    "        print(\"\\n‚úó Command 'ptxas' not found in path -- you might have to install `cudatoolkit-dev`\")\n",
    "    if not \"XLA_FLAGS\" in os.environ:\n",
    "        print(\"\\n‚úó XLA_FLAGS not set. If you encounter errors during training, you might have to set\")\n",
    "        print(\"        XLA_FLAGS=\\\"--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/\\\"\")\n",
    "    \n",
    "    # Allow growth of GPU memory, otherwise it will always look like all the memory is being used\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "else:\n",
    "    print(\"‚úó Tensorflow has NOT detected a GPU.\")\n",
    "    print()\n",
    "    print(\"For GPU support, visit: <https://www.tensorflow.org/install/pip>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Specification\n",
    "\n",
    "Your task in this assignment is to:\n",
    "\n",
    "1. Build an encoder‚Äîdecoder model based on recurrent neural networks.\n",
    "2. Train this model on the provided training data, a collection of parallel Swedish‚ÄìEnglish sentences.\n",
    "3. Evaluate the performance of this model on the provided test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX-5kU7mCus1"
   },
   "source": [
    "### The data: Swedish‚ÄìEnglish Anki corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luSjyoOqCus2"
   },
   "source": [
    "The data in this lab consists of bilingual Swedish‚ÄìEnglish sentence pairs from the [Tatoeba Project](https://tatoeba.org/en) as collected by [Anki](http://www.manythings.org/anki/).  These are comparatively short sentences, suitable for language learners, and therefore also well-suited for building a small machine translation model. Here are some example sentences from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EaFUyzF-Cus2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i returned to japan .', 'jag √•terv√§nde till japan .']\n",
      "['i love her .', 'jag √§lskar henne .']\n",
      "[\"this is tom ' s school .\", 'detta √§r toms skola .']\n",
      "['i can hardly stand .', 'jag kan knappt st√• .']\n"
     ]
    }
   ],
   "source": [
    "with open(\"en-sv-train.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        pair = [sent for sent in line.rstrip().split(\"\\t\")]\n",
    "        print(pair)\n",
    "        if i > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVYZFLVyCus3"
   },
   "source": [
    "Each line in the data files consists of an English‚ÄìSwedish sentence pair. The sentences are already lower-cased and pre-tokenized (using the [toktok tokenizer from NLTK](https://www.nltk.org/howto/tokenize.html)), so we can simply split them up by whitespace to get sequences of tokens.  To make your life a bit easier, we have removed sentences longer than 15 words. \n",
    "\n",
    "The next cell contains code that yields the sentences contained in a file as lists of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Eay_PCwACus4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'returned', 'to', 'japan', '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGLISH = 0\n",
    "SWEDISH = 1\n",
    "\n",
    "def sentences(filename, idx):\n",
    "    # Use idx=0 for English, idx=1 for Swedish\n",
    "    with open(filename, \"rt\", encoding=\"utf-8\") as source:\n",
    "        for line in source:\n",
    "            yield line.rstrip().split(\"\\t\")[idx].split()\n",
    "\n",
    "# Example usage\n",
    "next(sentences(\"en-sv-train.txt\", ENGLISH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypi63gGlCus5"
   },
   "source": [
    "## Part 1: Build the vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REbT89KqCus5"
   },
   "source": [
    "Before we can feed them into any model, we first need to convert the text strings to integers. For this purpose, we'll create a **vocabulary** of tokens that are known to the model, one vocabulary for each language. We need four **special tokens (or \"pseudowords\")**:\n",
    "\n",
    "1. `<pad>` at index 0 for padding purposes\n",
    "2. `<unk>` at index 1 to represent unknown words\n",
    "3. `<bos>` at index 2 to mark the \"beginning of sequence\" in the decoder\n",
    "4. `<eos>` at index 3 to mark the \"end of sequence\" in the decoder\n",
    "\n",
    "The remaining items in the vocabulary should be made up of the **most frequent words** in the training data for the respective language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ü§î Task 1: Write the function to build the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "R29hmyaRCus5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def make_vocab(sentences, max_size):\n",
    "    \"\"\"Return a list of the `max_size` most frequent tokens in `sentences`.\"\"\"\n",
    "\n",
    "    # TODO\n",
    "    # 1. Count of how often each word occurs in the data\n",
    "    # 2. Sort the words by their frequency, in descending order\n",
    "    # 3. Make a list of the special tokens plus the most frequent words, up to a length of `max_size`.\n",
    "    # 4. Return the list\n",
    "    \n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences.split()]\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            words.append(word)\n",
    "    word_counts = Counter(words)\n",
    "    sorted_words = []\n",
    "    for word,count in word_counts.most_common():\n",
    "        sorted_words.append(word)\n",
    "    \n",
    "    added_words = ['<pad>', '<unk>', '<bos>', '<eos>']\n",
    "\n",
    "    count = max_size - len(added_words)\n",
    "\n",
    "    final_words = added_words + sorted_words[:count]\n",
    "    \n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmr7Oe9cCus7"
   },
   "source": [
    "With this function, we can construct vocabularies containing the 5,000 most frequent words as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "onyml-oDCus7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_vocab = make_vocab(sentences('en-sv-train.txt', SWEDISH), 5000)\n",
    "tgt_vocab = make_vocab(sentences('en-sv-train.txt', ENGLISH), 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX9T47dnCus8"
   },
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "To test your code, check that each vocabulary contains 5,000 words, and includes the pseudowords at the right positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "tZBil9ezCus8",
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n"
     ]
    }
   ],
   "source": [
    "def test1():\n",
    "    assert len(src_vocab) == 5000\n",
    "    assert len(tgt_vocab) == 5000\n",
    "    assert src_vocab[:4] == ['<pad>', '<unk>', '<bos>', '<eos>']\n",
    "    assert tgt_vocab[:4] == ['<pad>', '<unk>', '<bos>', '<eos>']\n",
    "    print(\"All good!\")\n",
    "\n",
    "test1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the vocabularies in StringLookup layers\n",
    "\n",
    "For mapping tokens to their vocabulary IDs, we can use Keras' `StringLookup` layer. The next cell constructs layers for both the source and target vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_lookup_args = dict(output_mode=\"int\", mask_token=\"<pad>\", oov_token=\"<unk>\")\n",
    "src_lookup = layers.StringLookup(vocabulary=src_vocab, **string_lookup_args)\n",
    "tgt_lookup = layers.StringLookup(vocabulary=tgt_vocab, **string_lookup_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell gives an example how these `StringLookup` layers can be used. Note that the layers already return *tensors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   5 1005   11  530  188    4], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "example = \"i returned to japan yesterday .\".split()\n",
    "print(tgt_lookup(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§î Task 2: Sanity-check that these numbers are correct\n",
    "\n",
    "Check your understanding of what's happening in the `StringLookup` layer by writing two lines of code:\n",
    "1. One that prints the token corresponding to the _second integer_ in the tensor above.\n",
    "2. One that prints the integer corresponding to the _second word_ (\"returned\") in the example above.\n",
    "\n",
    "Use `tgt_vocab` directly for that, not the lookup layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "1005\n"
     ]
    }
   ],
   "source": [
    "#print(tgt_vocab[1])\n",
    "#word_index = tgt_vocab.index(\"returned\")\n",
    "#if(word_index.is_integer):\n",
    "#    print(word_index)\n",
    "#else:\n",
    "#    print(\"Word not found\")\n",
    "print(tgt_vocab[1])\n",
    "word_index = tgt_vocab.index(\"returned\")\n",
    "if(isinstance(word_index, int)):\n",
    "    print(word_index)\n",
    "else:\n",
    "    print(\"Word¬†not¬†found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2fQzgkPCus9"
   },
   "source": [
    "### Wrapping everything in data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MF4JRJVCus9"
   },
   "source": [
    "The next cell defines a function that wraps our dataset in TensorFlow's [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) API, which represents map-style datasets. The advantage of this is that it lets us use standard infrastructure related to the loading and automatic batching of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "TkFTb0LkCus9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_eos(tensor):\n",
    "    \"Helper function that appends '<eos>' to a sequence.\"\n",
    "    return tf.concat([tensor, tf.constant([\"<eos>\"], dtype=tf.string)], axis=0)\n",
    "\n",
    "def load_translation_dataset(src_lookup, tgt_lookup, filename):\n",
    "    # Build source dataset and convert with src_lookup\n",
    "    src_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        tf.ragged.constant(list(sentences(filename, SWEDISH)))\n",
    "    )\n",
    "    src_dataset = src_dataset.map(src_lookup)\n",
    "\n",
    "    # Build target dataset, append <eos> and convert with tgt_lookup\n",
    "    tgt_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        tf.ragged.constant(list(sentences(filename, ENGLISH)))\n",
    "    )\n",
    "    tgt_dataset = tgt_dataset.map(append_eos).map(tgt_lookup)\n",
    "    \n",
    "    # Zip them together\n",
    "    return tf.data.Dataset.zip((src_dataset, tgt_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnPlzxWrCus-"
   },
   "source": [
    "We load the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "t1tyWbfbCus-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = load_translation_dataset(src_lookup, tgt_lookup, \"en-sv-train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoUAW-GcCus_"
   },
   "source": [
    "The following function can be helpful for debugging. It extracts a single source‚Äìtarget pair of sentences from the specified *dataset* and converts it into batches of size&nbsp;1, which can be fed into the encoder‚Äìdecoder model. This also illustrates how the `Dataset` API works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "DC2j_oZyCus_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def example(dataset, i):\n",
    "    if i > 0:\n",
    "        dataset = dataset.skip(i-1)\n",
    "    return list(dataset.take(1).batch(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "dYhTBym7Cus_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[  6  10 245  15  20   7]], shape=(1, 6), dtype=int64)\n",
      "tf.Tensor([[ 29   9 477  55  28   8   3]], shape=(1, 7), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "x, y = example(train_dataset, 42)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw62gfhdCutA"
   },
   "source": [
    "## Part 2: The encoder‚Äìdecoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB280fFjCutA"
   },
   "source": [
    "In this section, you will implement the encoder‚Äìdecoder architecture, including the extension of that architecture by an attention mechanism. The implementation consists of four parts: the encoder, the attention mechanism, the decoder, and a class that wraps the complete architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUFFRKwGCutA"
   },
   "source": [
    "### Part 2.1: Implement the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKeHOZyxCutA"
   },
   "source": [
    "The encoder is a component that takes an input tensor of vocabulary IDs, like the `x` tensor from the example above, and performs the following steps:\n",
    "\n",
    "1. Look up **word embeddings** for each token in the sequence.\n",
    "2. Process them with a **bi-directional recurrent neural network**. This works with any type of RNN, but we will use **GRU (gated recurrent unit) layers** throughout this laboration.\n",
    "3. Feed the output through a linear layer. We also take the last hidden state of the forward GRU and the last hidden state of the backward GRU, concatenate them, and pass them through a linear layer. This produces a \"summary\" of the source sentence, which we will later feed into the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the encoder by defining it as a **custom Keras layer.** For this, we have to define a class that subclasses from `keras.layers.Layer`, instantiate all required model weights and/or (sub)layers in the `__init__()` function, and uses them to perform the layer's computation in the `call()` function. Below is some skeleton code to get you started; you can also [read more about making custom layers in the Keras Docs](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§î Task 3: Implement the encoder by completing the skeleton code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "klNT7vTPCutB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Encoder(layers.Layer):\n",
    "#     def __init__(self, num_words, embedding_dim=128, hidden_dim=256):\n",
    "#         super().__init__()\n",
    "#         # TODO: Add your code here that defines the required layers/weights\n",
    "#         self.embedding = ...\n",
    "#         self.rnn = ...\n",
    "#         self.linear = ...\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # TODO\n",
    "#         # 1. Look up the embeddings for the source words\n",
    "#         # 2. Apply a bi-directional GRU over the source sequences\n",
    "#         # 3. Apply a linear transformation to the GRU's output\n",
    "#         # 4. Concatenate forward + backward hidden states and apply a linear transformation on them too\n",
    "#         raise NotImplementedError\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_words, embedding_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        # TODO: Add your code here that defines the required layers/weights\n",
    "        self.embedding = layers.Embedding(input_dim=num_words, output_dim=embedding_dim)\n",
    "        self.rnn = layers.Bidirectional(layers.GRU(hidden_dim, return_sequences=True, return_state=True))\n",
    "        self.linear = layers.Dense(hidden_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO\n",
    "        # 1. Look up the embeddings for the source words\n",
    "        embeddings = self.embedding(inputs)        \n",
    "        # 2. Apply a bi-directional GRU over the source sequences\n",
    "        results = self.rnn(embeddings)\n",
    "        output = results[0]\n",
    "        forward = results[1]\n",
    "        backward = results[2]\n",
    "        # 3. Apply a linear transformation to the GRU's output\n",
    "        state = tf.concat([forward, backward], axis=-1)\n",
    "        # 4. Concatenate forward + backward hidden states and apply a linear transformation on them too\n",
    "        summary = self.linear(state)\n",
    "        return output, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut1QP_1hCutB"
   },
   "source": [
    "Your code must comply with the following specification:\n",
    "\n",
    "**__init__** (*num_words*, *embedding_dim* = 128, *hidden_dim* = 256)\n",
    "\n",
    "> Initialises the encoder. The encoder consists of an embedding layer that maps each of *num_words* words to an embedding vector of size *embedding_dim*, a bidirectional GRU that maps each embedding vector to a position-specific representation of size 2 √ó *hidden_dim*, and a final linear layer that projects these representations to new representations of size *hidden_dim*.\n",
    "\n",
    "**call** (*self*, *inputs*)\n",
    "\n",
    "> Takes a tensor *inputs* with source-language word ids and sends it through the encoder. The input tensor has shape (*batch_size*, *src_len*), where *src_len* is the length of the sentences in the batch. (We will make sure that all sentences in the same batch have the same length.) The method returns a pair of tensors (*output*, *hidden*), where *output* has shape (*batch_size*, *src_len*, *hidden_dim*), and *hidden* has shape (*batch_size*, *hidden_dim*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xybpq39KCutC"
   },
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "To test your code, instantiate an encoder, feed it the first source sentence in the training data, and check that the tensors returned by the encoder have the expected shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "LU4SIh0NCutC",
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 512)\n",
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "def test21():\n",
    "    src, tgt = example(train_dataset, 42)\n",
    "    encoder = Encoder(src_lookup.vocabulary_size())\n",
    "    output, hidden = encoder(src)\n",
    "    print(output.shape)  # should be (batch_size, src_len, hidden_dim)\n",
    "    print(hidden.shape)  # should be (batch_size, hidden_dim)\n",
    "\n",
    "test21()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnD4At5uCutD"
   },
   "source": [
    "### Part 2.2: Implement the attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twmxl69tCutD"
   },
   "source": [
    "Your next task is to implement the attention mechanism. Recall that the purpose of this mechanism is to inform the decoder when generating the translation of the next word. For this, attention has access to the previous hidden state of the decoder, as well as the complete output of the encoder. It returns the attention-weighted sum of the encoder output, the so-called *context* vector. For later usage, we also return the attention weights.\n",
    "\n",
    "As mentioned in the lecture, attention can be implemented in various ways. One very simple implementation is *uniform attention*, which assigns equal weight to each position-specific representation in the output of the encoder, and completely ignores the hidden state of the decoder. This mechanism is implemented in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "VK4LsCUfCutD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UniformAttention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, decoder_hidden, encoder_output, mask=None):\n",
    "        # Set all attention scores to the same constant value (0). After\n",
    "        # the softmax, we will have uniform weights.\n",
    "        scores = tf.zeros_like(encoder_output[:, :, -1])\n",
    "        \n",
    "        # Mask out the attention scores for the padding tokens. We set\n",
    "        # them to -inf. After the softmax, we will have 0.\n",
    "        if mask is not None:\n",
    "            masked_value = -float('inf') * tf.ones_like(scores)\n",
    "            scores = tf.where(mask, scores, masked_value)\n",
    "        \n",
    "        # Convert scores into weights\n",
    "        alpha = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        # The context is the alpha-weighted sum of the encoder outputs.\n",
    "        context = tf.linalg.matmul(tf.expand_dims(alpha, axis=1), encoder_output)\n",
    "        context = tf.squeeze(context, axis=1)\n",
    "        \n",
    "        return alpha, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noYs0hSHCutE"
   },
   "source": [
    "One technical detail in this code is our use of *mask* to compute attention weights only for the ‚Äòreal‚Äô tokens in the source sentences, but not for the padding tokens that we introduce to bring all sentences in a batch to the same length.\n",
    "\n",
    "Your task now is to implement the attention mechanism from the paper by [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473). The relevant equation is in Section&nbsp;A.1.2:\n",
    "\n",
    "$$\n",
    "a(s_{i-1}, h_j) = v^{\\top} \\tanh(W s_{i-1} + U h_j)\n",
    "$$\n",
    "\n",
    "This equation specifies how to compute the attention score (a scalar) for the previous hidden state of the decoder, denoted by $s_{i-1}$, and the $j$-th position-specific representation in the output of the encoder, denoted by $h_j$. The equation refers to three parameters: a vector $v$ and $W$ and $U$. In PyTorch, these parameters can be represented in terms of (bias-free) linear layers that are trained along with the other parameters of the model.\n",
    "\n",
    "Here is the skeleton code for this problem. As you can see, your specific task is to initialise the required parameters and to compute the attention scores (*scores*); the rest of the code is the same as for the uniform attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§î Task 4: Implement Bahdanau attention by completing the skeleton code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "RyclW2osCutE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BahdanauAttention(layers.Layer):\n",
    "#     def __init__(self, hidden_dim=256):\n",
    "#         super().__init__()\n",
    "#         self.supports_masking = True\n",
    "#         # TODO: Add your code here that defines the required layers/weights\n",
    "#         self.w = ...\n",
    "#         self.u = ...\n",
    "#         self.v = ...\n",
    "\n",
    "#     def call(self, decoder_hidden, encoder_output, mask=None):\n",
    "#         # TODO: Replace the next line with your own code that computes the attention scores\n",
    "#         scores = tf.zeros_like(encoder_output[:, :, -1])\n",
    "\n",
    "#         # ... The rest of the code is as in UniformAttention ‚Äî NO NEED TO MODIFY BELOW THIS LINE!\n",
    "\n",
    "#         # Mask out the attention scores for the padding tokens. We set\n",
    "#         # them to -inf. After the softmax, we will have 0.\n",
    "#         if mask is not None:\n",
    "#             masked_value = -float('inf') * tf.ones_like(scores)\n",
    "#             scores = tf.where(mask, scores, masked_value)\n",
    "        \n",
    "#         # Convert scores into weights\n",
    "#         alpha = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "#         # The context is the alpha-weighted sum of the encoder outputs.\n",
    "#         context = tf.linalg.matmul(tf.expand_dims(alpha, axis=1), encoder_output)\n",
    "#         context = tf.squeeze(context, axis=1)\n",
    "        \n",
    "#         return alpha, context\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.supports_masking = True\n",
    "        # TODO: Add your code here that defines the required layers/weights\n",
    "        self.w = layers.Dense(hidden_dim)  # For encoder outputs\n",
    "        self.u = layers.Dense(hidden_dim)\n",
    "        self.v = layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_hidden, encoder_output, mask=None):\n",
    "        # TODO: Replace the next line with your own code that computes the attention scores\n",
    "        hidden_decoder = tf.expand_dims(decoder_hidden, axis=1)\n",
    "        encoder = self.w(encoder_output)\n",
    "        decoder = self.u(hidden_decoder)\n",
    "        res = tf.nn.tanh(encoder + decoder)\n",
    "        scores = tf.squeeze(self.v(res), axis=-1)\n",
    "\n",
    "        # ... The rest of the code is as in UniformAttention ‚Äî NO NEED TO MODIFY BELOW THIS LINE!\n",
    "\n",
    "        # Mask out the attention scores for the padding tokens. We set\n",
    "        # them to -inf. After the softmax, we will have 0.\n",
    "        if mask is not None:\n",
    "            masked_value = -float('inf') * tf.ones_like(scores)\n",
    "            scores = tf.where(mask, scores, masked_value)\n",
    "        \n",
    "        # Convert scores into weights\n",
    "        alpha = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        # The context is the alpha-weighted sum of the encoder outputs.\n",
    "        context = tf.linalg.matmul(tf.expand_dims(alpha, axis=1), encoder_output)\n",
    "        context = tf.squeeze(context, axis=1)\n",
    "        \n",
    "        return alpha, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTB0RPfDCutF"
   },
   "source": [
    "Your code must comply with the following specification:\n",
    "\n",
    "**call** (*decoder_hidden*, *encoder_output*, *mask*)\n",
    "\n",
    "> Takes the previous hidden state of the decoder (*decoder_hidden*) and the encoder output (*encoder_output*) and returns a pair (*alpha*, *context*) where *context* is the context as computed as in [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473), and *alpha* are the corresponding attention weights. The hidden state has shape (*batch_size*, *hidden_dim*), the encoder output has shape (*batch_size*, *src_len*, *hidden_dim*), the context has shape (*batch_size*, *hidden_dim*), and the attention weights have shape (*batch_size*, *src_len*).\n",
    "\n",
    "#### üí° Hints on the implementation\n",
    "\n",
    "You may need a few more \"low-level\" TensorFlow functions to implement this part, concretely:\n",
    "    \n",
    "- `tf.expand_dims()` and `tf.squeeze()` to add/remove a dimension from a tensor. This is because some tensors have a \"timestep\" dimension while others (e.g. the hidden state of the decoder) don't.\n",
    "- `tf.nn.tanh()` to compute the $\\tanh$ function on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxNwwlmuCutF"
   },
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "To test your code, extend your test from Task 3: Feed the output of your encoder into your attention class. As the previous hidden state of the decoder, you can use the hidden state returned by the encoder. Later, you don't need to pass the mask explicitly (Keras will do this automatically), but for testing purposes, you can obtain the mask from a layer's output via `output._keras_mask`.\n",
    "\n",
    "Check that the context tensor and the attention weights returned by the attention class have the expected shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ZUqLpHXiCutG",
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n",
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "def test22():\n",
    "    src, tgt = example(train_dataset, 42)\n",
    "    encoder = Encoder(src_lookup.vocabulary_size())\n",
    "    output, hidden = encoder(src)\n",
    "    attention = BahdanauAttention()\n",
    "    alpha, context = attention(hidden, output, mask=output._keras_mask)\n",
    "    print(alpha.shape)    # should be (batch_size, src_len)\n",
    "    print(context.shape)  # should be (batch_size, hidden_dim)\n",
    "\n",
    "test22()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fB2A4Qa1CutG"
   },
   "source": [
    "### Part 2.3: Implement the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO_4A4XXCutH"
   },
   "source": [
    "Now you are ready to implement the decoder. Like the encoder, the decoder is based on a GRU; but this time we use a unidirectional network, as we generate the target sentences left-to-right.\n",
    "\n",
    "**‚ö†Ô∏è We expect that solving this problem will take you the longest time in this lab.**\n",
    "\n",
    "Because the decoder is an autoregressive model, we need to unroll the GRU \"manually\": At each position, we take the previous hidden state as well as the new input, and apply the GRU for one step. The initial hidden state comes from the encoder. The new input is the embedding of the previous word, concatenated with the context vector from the attention model. To produce the final output, we take the output of the GRU, concatenate the embedding vector and the context vector (residual connection), and feed the result into a linear layer. Here is a graphical representation:\n",
    "\n",
    "<img src=\"https://gitlab.liu.se/nlp/nlp-course/-/raw/master/labs/l3/decoder.svg\" width=\"50%\" alt=\"Decoder architecture\"/>\n",
    "\n",
    "We need to implement this manual unrolling for two very similar tasks: When *training*, both the inputs to and the target outputs of the GRU come from the training data. When *decoding*, the outputs of the GRU are used to generate new target-side words, and these words become the inputs to the next step of the unrolling. **We have already implemented the `call` method that handles both these two different modes of usage ‚Äî you don't need to modify this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§î Task 5: Implement the `step` method that takes a single step with the GRU\n",
    "\n",
    "You will also need to add any necessary layers/weights that you use in the `__init__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "xBDjsJ-1CutH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Decoder(layers.Layer):\n",
    "#     def __init__(self, trg_lookup, attention, embedding_dim=128, hidden_dim=256, max_len=16):\n",
    "#         super().__init__()\n",
    "#         num_words = trg_lookup.vocabulary_size()\n",
    "#         self.embedding = layers.Embedding(num_words, embedding_dim, mask_zero=True)\n",
    "#         self.bos_index = trg_lookup([\"<bos>\"]).numpy()\n",
    "#         self.max_len = max_len\n",
    "#         # TODO: Add your code here that defines the required layers/weights\n",
    "#         self.attention = ...\n",
    "#         self.rnn_cell = ...\n",
    "#         self.linear = ...\n",
    "\n",
    "#     def call(self, encoder_output, initial_state, targets=None, training=False, mask=None):\n",
    "#         # YOU WON'T NEED TO MODIFY ANYTHING IN THIS FUNCTION.\n",
    "\n",
    "#         if training:\n",
    "#             assert targets is not None\n",
    "\n",
    "#         # Initialise the hidden state from `initial_state`\n",
    "#         state = initial_state\n",
    "        \n",
    "#         # Initialise the decoder input with the `<bos>` symbol\n",
    "#         next_input = self.bos_index * tf.ones_like(initial_state, dtype=tf.int64)\n",
    "#         next_input = next_input[:, 0]\n",
    "        \n",
    "#         # Initialise the list of outputs and attention weights\n",
    "#         outputs = tf.TensorArray(\n",
    "#             tf.float32,\n",
    "#             size=0 if training else self.max_len,\n",
    "#             dynamic_size=training,\n",
    "#         )\n",
    "#         alphas = tf.TensorArray(\n",
    "#             tf.float32,\n",
    "#             size=0 if training else self.max_len,\n",
    "#             dynamic_size=training,\n",
    "#         )\n",
    "#         inputs = tf.TensorArray(\n",
    "#             tf.int64,\n",
    "#             size=0 if training else self.max_len,\n",
    "#             dynamic_size=training,\n",
    "#         )\n",
    "\n",
    "#         # In training mode, we iterate over the length of the target sentences,\n",
    "#         # otherwise we iterate until `self.max_len` is reached\n",
    "#         max_len = tf.shape(targets)[1] if training else self.max_len\n",
    "        \n",
    "#         for i in range(max_len):\n",
    "#             # In training mode, we feed the correct (gold) predictions as the next input\n",
    "#             if training and i > 0:\n",
    "#                 next_input = targets[:, i-1]\n",
    "            \n",
    "#             # Get the embedding for the previous word\n",
    "#             prev_embed = self.embedding(next_input)\n",
    "            \n",
    "#             # Take one step with the RNN\n",
    "#             step_output, state, alpha = self.step(encoder_output, state, prev_embed, mask=mask)\n",
    "            \n",
    "#             # Update the list of generated words and attention weights\n",
    "#             outputs = outputs.write(i, step_output)\n",
    "#             alphas = alphas.write(i, alpha)\n",
    "#             inputs = inputs.write(i, next_input)\n",
    "\n",
    "#             # Set the prediction with highest probability as the input for the next timestep\n",
    "#             if not training:\n",
    "#                 next_input = tf.math.argmax(step_output, axis=-1)\n",
    "\n",
    "#         # Lists of outputs and attention weights are [tgt_len, batch_size, *],\n",
    "#         # so we transpose them to have the batch dimension in first place again.\n",
    "#         outputs = tf.transpose(outputs.stack(), perm=[1,0,2])\n",
    "#         alphas = tf.transpose(alphas.stack(), perm=[1,0,2])\n",
    "#         inputs = tf.transpose(inputs.stack(), perm=[1,0])\n",
    "#         outputs._keras_mask = (inputs != 0)\n",
    "        \n",
    "#         return outputs, alphas\n",
    "    \n",
    "#     def step(self, encoder_output, hidden_state, prev_embed, mask=None):\n",
    "#         # TODO: Replace the next line with your own code; this should follow the illustration above.\n",
    "#         # 1. Get the attention weights and context vector\n",
    "#         alpha, context = ...\n",
    "#         # 2. Concatenate the inputs for the GRU\n",
    "#         rnn_input = ...\n",
    "#         # 3. Take one step with the GRU cell\n",
    "#         rnn_output, hidden_state = ...\n",
    "#         # 4. Concatenate the respective tensors to produce the final output\n",
    "#         output = ...\n",
    "        \n",
    "#         return output, hidden_state, alpha\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, trg_lookup, attention, embedding_dim=128, hidden_dim=256, max_len=16):\n",
    "        super().__init__()\n",
    "        num_words = trg_lookup.vocabulary_size()\n",
    "        self.embedding = layers.Embedding(num_words, embedding_dim, mask_zero=True)\n",
    "        self.bos_index = trg_lookup([\"<bos>\"]).numpy()\n",
    "        self.max_len = max_len\n",
    "        # TODO: Add your code here that defines the required layers/weights\n",
    "        self.attention = attention\n",
    "        self.rnn_cell = layers.GRUCell(hidden_dim)\n",
    "        self.linear = layers.Dense(num_words)\n",
    "\n",
    "    def call(self, encoder_output, initial_state, targets=None, training=False, mask=None):\n",
    "        # YOU WON'T NEED TO MODIFY ANYTHING IN THIS FUNCTION.\n",
    "\n",
    "        if training:\n",
    "            assert targets is not None\n",
    "\n",
    "        # Initialise the hidden state from `initial_state`\n",
    "        state = initial_state\n",
    "        \n",
    "        # Initialise the decoder input with the `<bos>` symbol\n",
    "        next_input = self.bos_index * tf.ones_like(initial_state, dtype=tf.int64)\n",
    "        next_input = next_input[:, 0]\n",
    "        \n",
    "        # Initialise the list of outputs and attention weights\n",
    "        outputs = tf.TensorArray(\n",
    "            tf.float32,\n",
    "            size=0 if training else self.max_len,\n",
    "            dynamic_size=training,\n",
    "        )\n",
    "        alphas = tf.TensorArray(\n",
    "            tf.float32,\n",
    "            size=0 if training else self.max_len,\n",
    "            dynamic_size=training,\n",
    "        )\n",
    "        inputs = tf.TensorArray(\n",
    "            tf.int64,\n",
    "            size=0 if training else self.max_len,\n",
    "            dynamic_size=training,\n",
    "        )\n",
    "\n",
    "        # In training mode, we iterate over the length of the target sentences,\n",
    "        # otherwise we iterate until `self.max_len` is reached\n",
    "        max_len = tf.shape(targets)[1] if training else self.max_len\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            # In training mode, we feed the correct (gold) predictions as the next input\n",
    "            if training and i > 0:\n",
    "                next_input = targets[:, i-1]\n",
    "            \n",
    "            # Get the embedding for the previous word\n",
    "            prev_embed = self.embedding(next_input)\n",
    "            \n",
    "            # Take one step with the RNN\n",
    "            step_output, state, alpha = self.step(encoder_output, state, prev_embed, mask=mask)\n",
    "            \n",
    "            # Update the list of generated words and attention weights\n",
    "            outputs = outputs.write(i, step_output)\n",
    "            alphas = alphas.write(i, alpha)\n",
    "            inputs = inputs.write(i, next_input)\n",
    "\n",
    "            # Set the prediction with highest probability as the input for the next timestep\n",
    "            if not training:\n",
    "                next_input = tf.math.argmax(step_output, axis=-1)\n",
    "\n",
    "        # Lists of outputs and attention weights are [tgt_len, batch_size, *],\n",
    "        # so we transpose them to have the batch dimension in first place again.\n",
    "        outputs = tf.transpose(outputs.stack(), perm=[1,0,2])\n",
    "        alphas = tf.transpose(alphas.stack(), perm=[1,0,2])\n",
    "        inputs = tf.transpose(inputs.stack(), perm=[1,0])\n",
    "        outputs._keras_mask = (inputs != 0)\n",
    "        \n",
    "        return outputs, alphas\n",
    "    \n",
    "    def step(self, encoder_output, hidden_state, prev_embed, mask=None):\n",
    "        # TODO: Replace the next line with your own code; this should follow the illustration above.\n",
    "        # 1. Get the attention weights and context vector\n",
    "        alpha, context = self.attention(hidden_state, encoder_output, mask=mask)\n",
    "        # 2. Concatenate the inputs for the GRU\n",
    "        rnn_input =  tf.concat([prev_embed, context], axis=-1)\n",
    "        # 3. Take one step with the GRU cell\n",
    "        rnn_output, hidden_state = self.rnn_cell(rnn_input, hidden_state)\n",
    "        # 4. Concatenate the respective tensors to produce the final output\n",
    "        output = self.linear(tf.concat([rnn_output, context], axis=-1))\n",
    "        \n",
    "        return output, hidden_state, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej2-lzyVCutH"
   },
   "source": [
    "Your implementation should comply with the following specification:\n",
    "\n",
    "**step** (*self*, *encoder_output*, *hidden*, *prev_embed*, *mask*)\n",
    "\n",
    "> Performs a single step in the manual unrolling of the decoder GRU. This takes the output of the encoder (*encoder_output*), the previous hidden state of the decoder (*hidden*), the embedding vector of the previous word (*prev_embed*), and the source mask as described in Problem&nbsp;2.2 (*mask*), and computes the output as described above.\n",
    ">\n",
    "> The shape of *encoder_output* is (*batch_size*, *src_len*, *hidden_dim*); the shape of *hidden* is (*batch_size*, *hidden_dim*); the shape of *src_mask* is (*batch_size*, *src_len*); and the shape of *prev_embed* is (*batch_size*, *embedding_dim*).\n",
    ">\n",
    "> The method returns a triple of tensors (*output*, *hidden*, *alpha*) where *output* is the position-specific output of the GRU, of shape (*batch_size*, *num_words*); *hidden* is the new hidden state, of shape (*batch_size*, *hidden_dim*); and *alpha* are the attention weights that were used to compute the *output*, of shape (*batch_size*, *src_len*).\n",
    "\n",
    "#### üí° Hints on the implementation\n",
    "\n",
    "**GRU vs. GRUCell.** In Keras, an RNN layer like `GRU` is used to process an entire sequence. A single *time-step* of a sequence is handled by a [`GRUCell`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell) instead. You can think of a `GRU` layer as functionally equivalent to a for-loop around a `GRUCell`. Since we want to perform the RNN steps individually for this model, you should use a [`GRUCell`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell) instead of a `GRU` layer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFvNiX-YCutI"
   },
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "To test your code, extend your test from the previous problems, and simulate a complete forward pass of the encoder‚Äìdecoder architecture on the example sentence. Check the shapes of the resulting tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "65lLG-bBCutI",
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16, 5000)\n",
      "(4, 16, 5)\n",
      "(4, 8, 5000)\n"
     ]
    }
   ],
   "source": [
    "def test23():\n",
    "    src, tgt = list(train_dataset.take(4).padded_batch(4))[0]\n",
    "    encoder = Encoder(src_lookup.vocabulary_size())\n",
    "    encoder_output, hidden = encoder(src)\n",
    "    attention = BahdanauAttention()\n",
    "    decoder = Decoder(tgt_lookup, attention)\n",
    "    decoded, alphas = decoder(encoder_output, hidden)\n",
    "    print(decoded.shape)  # should be (batch_size, max_len, vocabulary_size)\n",
    "    print(alphas.shape)   # should be (batch_size, max_len, src_len)\n",
    "    decoded, _ = decoder(encoder_output, hidden, targets=tgt, training=True)\n",
    "    print(decoded.shape)  # should be (batch_size, tgt_len, vocabulary_size)\n",
    "\n",
    "test23()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70yHTFS9CutJ"
   },
   "source": [
    "### Encoder‚ÄìDecoder wrapper class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeKfvES5CutJ"
   },
   "source": [
    "The last part of the implementation is a class that wraps the encoder and the decoder as a single model.  We also implement a custom `train_step` function so that the gold targets will get passed to the decoder during training, and a custom `test_step` function to make sure the decoded sequences and the gold sequences are padded to the same length before computing losses and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "lUjIGniZCutJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(keras.Model):\n",
    "    def __init__(self, src_lookup, tgt_lookup, embedding_dim=128, hidden_dim=256, max_len=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = Encoder(\n",
    "            src_lookup.vocabulary_size(),\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dim=hidden_dim\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            tgt_lookup,\n",
    "            BahdanauAttention(hidden_dim=hidden_dim),\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            max_len=max_len,\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training=False, targets=None):\n",
    "        x_out, x_hidden = self.encoder(inputs, training=training)\n",
    "        outputs, alphas = self.decoder(x_out, x_hidden, training=training, targets=targets)\n",
    "        if training:\n",
    "            return outputs\n",
    "        else:\n",
    "            return outputs, alphas\n",
    "    \n",
    "    # Following <https://keras.io/guides/customizing_what_happens_in_fit/>\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Here we supply \"targets\" so that the decoder has access to it\n",
    "            y_pred = self(x, training=True, targets=y)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred, _alphas = self(x, training=False)\n",
    "\n",
    "        # Pad sequences to the same number of time-steps\n",
    "        max_len = tf.math.maximum(tf.shape(y)[1], tf.shape(y_pred)[1])\n",
    "        y_pad = [[0, 0], [0, max_len - tf.shape(y)[1]]]\n",
    "        y = tf.pad(y, y_pad)\n",
    "        y_pred_pad = [[0, 0], [0, max_len - tf.shape(y_pred)[1]], [0, 0]]\n",
    "        y_pred = tf.pad(y_pred, y_pred_pad)\n",
    "            \n",
    "        self.compute_loss(x, y, y_pred, None)\n",
    "        return self.compute_metrics(x, y, y_pred, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3uW3TKACutK"
   },
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "As a final test, instantiate an encoder‚Äìdecoder model and use it to decode the example sentence. Check the shapes of the resulting tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "oO6zONDwCutK",
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 5000)\n",
      "(1, 16, 6)\n"
     ]
    }
   ],
   "source": [
    "def test24():\n",
    "    src, tgt = example(train_dataset, 42)\n",
    "    encoder_decoder = EncoderDecoder(src_lookup, tgt_lookup)\n",
    "    outputs, alphas = encoder_decoder(src)\n",
    "    print(outputs.shape)  # should be (batch_size, max_len, vocabulary_size)\n",
    "    print(alphas.shape)   # should be (batch_size, max_len, src_len)\n",
    "\n",
    "test24()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laK9bbq8CutL"
   },
   "source": [
    "## Part 3: Train a translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q68aWc_2CutL"
   },
   "source": [
    "We now have all the pieces to build and train a complete translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5MYw2PJCutL"
   },
   "source": [
    "### Translator class\n",
    "\n",
    "We first define a class `Translator` that initialises an encoder‚Äìdecoder model and uses it to translate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Evi-J7BfCutM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self, src_lookup, tgt_lookup, batch_size=32, **kwargs):\n",
    "        self.src_lookup = src_lookup\n",
    "        self.tgt_lookup = tgt_lookup\n",
    "        self.model = EncoderDecoder(src_lookup, tgt_lookup, **kwargs)\n",
    "        self.tgt_vocab = tgt_lookup.get_vocabulary()\n",
    "        self.eos_index = self.tgt_vocab.index(\"<eos>\")\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def compile(self, *args, **kwargs):\n",
    "        return self.model.compile(*args, **kwargs)\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "\n",
    "    def translate(self, sentences, return_alphas=False):\n",
    "        \"\"\"This function takes sentences and returns their translation as a string.\n",
    "        \n",
    "        `sentences` can be either:\n",
    "          - A tf.data.Dataset object\n",
    "          - A list of strings\n",
    "        \"\"\"\n",
    "        if isinstance(sentences, tf.data.Dataset):\n",
    "            inputs = sentences\n",
    "        elif isinstance(sentences, (list, tuple)):\n",
    "            inputs = (\n",
    "                tf.data.Dataset.from_tensor_slices(\n",
    "                    tf.ragged.constant([x.split() for x in sentences])\n",
    "                )\n",
    "                .map(self.src_lookup)\n",
    "                .padded_batch(min(self.batch_size, len(sentences)))\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"'sentences' should be either a tf.Dataset or a list of strings; got: {type(sentences)}\")\n",
    "\n",
    "        outputs, alphas = self.model.predict(inputs, verbose=0)\n",
    "        outputs = tf.math.argmax(outputs, axis=-1).numpy().tolist()\n",
    "        try:\n",
    "            alphas = alphas.numpy().tolist()\n",
    "        except AttributeError:\n",
    "            alphas = alphas.tolist()\n",
    "        generated = []\n",
    "\n",
    "        for y_pred, alpha in zip(outputs, alphas):\n",
    "            try:\n",
    "                eos_idx = y_pred.index(self.eos_index)\n",
    "                del y_pred[eos_idx:]\n",
    "                del alpha[eos_idx:]\n",
    "            except ValueError:\n",
    "                pass\n",
    "            tokens = [self.tgt_vocab[idx] for idx in y_pred if idx > 0]\n",
    "            tokens = \" \".join(tokens)\n",
    "            if return_alphas:\n",
    "                generated.append((tokens, alpha))\n",
    "            else:\n",
    "                generated.append(tokens)\n",
    "\n",
    "        return generated\n",
    "        \n",
    "    def translate_with_attention(self, sentences):\n",
    "        return self.translate(sentences, return_alphas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2660iyt-CutN"
   },
   "source": [
    "The code below shows how this class is supposed to be used (its output will be nonsensical right now, of course, since the model hasn't been trained yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "NFiJLKRXCutN",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cloud 1985 insurance seemed seemed held held held effect fallen 1985 insurance seemed seemed held held',\n",
       " 'fairly fit lucid belongs insulated doodled lucid belongs insulated harassing during heart-shaped mistaken doodled fairly lucid']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator(src_lookup, tgt_lookup)\n",
    "# Alternative \"mini\" version of the model for testing:\n",
    "#translator = Translator(src_lookup, tgt_lookup, embedding_dim=32, hidden_dim=64, batch_size=16, max_len=8)\n",
    "translator.translate(['st√§ng av vattnet .', 'jag √§lskar friterade bananer .'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cFT9atlCutO"
   },
   "source": [
    "### Evaluation function\n",
    "\n",
    "As mentioned in the lecture, machine translation systems are typically evaluated using the **BLEU metric**. Here we use the implementation of this metric from the `sacrebleu` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "bleu_params = dict(effective_order=True, tokenize=\"none\", force=True, smooth_method=\"floor\", smooth_value=0.01)\n",
    "bleu = BLEU(**bleu_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the test sentence is exactly identical to the reference sentence, the score will be 100 (plus/minus potential floating point rounding errors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00000000000004"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.sentence_score(\"the house is blue .\", [\"the house is blue .\"]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change some words, the score will go down, though never below zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9763536438352522"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.sentence_score(\"the house was red .\", [\"the house is blue .\"]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a helper function that takes a trained `Translator` model as well as a `Dataset`, runs all sentences through the translator, and computes the BLEU score for the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_bleu(translator, dataset):\n",
    "    hyp = translator.translate(dataset)\n",
    "    ref = [\n",
    "        \" \".join(translator.tgt_vocab[idx] for idx in s if idx not in (0, translator.eos_index))\n",
    "        for s in dataset.unbatch().map(lambda _, x: x).as_numpy_iterator()\n",
    "    ]\n",
    "    return bleu.corpus_score(hyp, [ref]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ozk1hurqCutP"
   },
   "source": [
    "We want to report the BLEU score on the **validation data**, so let's load this as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "HNtBMDVqCutP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_dataset = load_translation_dataset(src_lookup, tgt_lookup, \"en-sv-valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe2rPzwmCutQ"
   },
   "source": [
    "### Batching\n",
    "\n",
    "So far we only tested our code on \"batches\" with a single sentence. In order to use larger batches, we need to make sure that all of the sentences in a batch have the same length. We achieve this by _padding_ the shorter sentences to the length of the longest one. Luckily, the `tf.Dataset` class has a function [`padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) that will do this for us. If we provide a `Dataset` for training, Keras won't shuffle the data automatically, so we also have to [`shuffle`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) the dataset explicitly. (For validation, shuffling doesn't matter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "X-yfmnSzCutQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batched = train_dataset.shuffle(512).padded_batch(64)\n",
    "valid_batched = valid_dataset.padded_batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfM1Ov-sCutQ"
   },
   "source": [
    "### Training\n",
    "\n",
    "Training works as for any other Keras model: we first need to `compile` the model with the optimizer, loss function, and validation metrics that we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "yr4VP-SpCutQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=2e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgXElolrCutR"
   },
   "source": [
    "Now it is time to train the system. During training, these diagnostics will be updated periodically: the running average of the training loss; after a full epoch, the loss and the BLEU score on the validation data will be computed and printed.\n",
    "\n",
    "Let's also define a callback that additionally prints the translation of a sample sentence, *jag saknar min familj* (which should translate into *i miss my family*), every 50 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    keras.callbacks.LambdaCallback(\n",
    "        on_train_batch_end=lambda b, _: tf.print(\" - jag saknar min familj . ->\", translator.translate(['jag saknar min familj .'])[0]) if b > 0 and b % 50 == 0 else None,\n",
    "        on_epoch_end=lambda _, l: l.__setitem__(\"val_bleu\", compute_bleu(translator, valid_batched))\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§î Task 6: Run the model training\n",
    "\n",
    "Run the following code cells that train the model and evaluate it on the validation data.\n",
    "\n",
    "Training the translator takes quite a bit of compute power and time. The default number of epochs is 2; however, you may want to try training for longer, or interrupt the training prematurely and use a partially trained model in case you run out of time.\n",
    "\n",
    "**‚ö†Ô∏è Your submitted notebook must contain output demonstrating at least 20 BLEU points on the validation data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "VW8wF8oRCutR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - jag saknar min familj . -> you .....] - ETA: 1:13 - loss: 5.2431 \n",
      " - jag saknar min familj . -> you the shortcut .1:06 - loss: 4.6267\n",
      " - jag saknar min familj . -> i ' m a lot .ETA: 50s - loss: 4.2751 \n",
      " - jag saknar min familj . -> i ' m not a lot . 35s - loss: 4.0329\n",
      " - jag saknar min familj . -> i ' m not <unk> . 20s - loss: 3.8325\n",
      " - jag saknar min familj . -> i ' m not a <unk> .s - loss: 3.6635 \n",
      "322/322 [==============================] - 112s 328ms/step - loss: 3.5986 - val_loss: 4.4430 - val_bleu: 5.1937\n",
      "Epoch 2/5\n",
      " - jag saknar min familj . -> i ' m not my friend .1 - loss: 2.7613\n",
      " - jag saknar min familj . -> i ' m not a little .04 - loss: 2.6763\n",
      " - jag saknar min familj . -> i can ' t do .TA: 50s - loss: 2.6063 \n",
      " - jag saknar min familj . -> i ' m my friend . 35s - loss: 2.5458\n",
      " - jag saknar min familj . -> i like my car .A: 20s - loss: 2.4832\n",
      " - jag saknar min familj . -> i love my hair .: 6s - loss: 2.4214 \n",
      "322/322 [==============================] - 100s 311ms/step - loss: 2.3956 - val_loss: 4.3541 - val_bleu: 12.6948\n",
      "Epoch 3/5\n",
      " - jag saknar min familj . -> i love my coat .: 1:23 - loss: 2.0644\n",
      " - jag saknar min familj . -> i hate my bag .A: 1:06 - loss: 2.0040\n",
      " - jag saknar min familj . -> i hate my shoes . 51s - loss: 1.9535 \n",
      " - jag saknar min familj . -> i just clipped my car . loss: 1.9080\n",
      " - jag saknar min familj . -> i hate my car .A: 21s - loss: 1.8568\n",
      " - jag saknar min familj . -> i already already already seen .056 \n",
      "322/322 [==============================] - 104s 322ms/step - loss: 1.7841 - val_loss: 4.5444 - val_bleu: 20.0986\n",
      "Epoch 4/5\n",
      " - jag saknar min familj . -> i already already already my car .828\n",
      " - jag saknar min familj . -> i already hate my friends .ss: 1.4305\n",
      " - jag saknar min familj . -> i found my car .: 51s - loss: 1.3903 \n",
      " - jag saknar min familj . -> i miss my family .36s - loss: 1.3445\n",
      " - jag saknar min familj . -> i miss my family .21s - loss: 1.2994\n",
      " - jag saknar min familj . -> i miss my family .6s - loss: 1.2595 \n",
      "322/322 [==============================] - 101s 315ms/step - loss: 1.2401 - val_loss: 4.5372 - val_bleu: 29.8653\n",
      "Epoch 5/5\n",
      " - jag saknar min familj . -> i miss my family .1:20 - loss: 0.9608\n",
      " - jag saknar min familj . -> i hate my family .1:04 - loss: 0.9261\n",
      " - jag saknar min familj . -> i miss my family .49s - loss: 0.8971 \n",
      " - jag saknar min familj . -> i miss my family .35s - loss: 0.8674\n",
      " - jag saknar min familj . -> i miss my family .20s - loss: 0.8397\n",
      " - jag saknar min familj . -> i miss my family .6s - loss: 0.8156 \n",
      "322/322 [==============================] - 100s 311ms/step - loss: 0.8039 - val_loss: 4.5290 - val_bleu: 35.7567\n",
      "CPU times: user 38min 36s, sys: 1min 54s, total: 40min 30s\n",
      "Wall time: 8min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    translator.fit(train_batched, epochs=5, validation_data=valid_batched, callbacks=my_callbacks)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.7567402809576"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(translator, valid_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ÑπÔ∏è Some notes on the translations\n",
    "\n",
    "If you try out sentences to see their translation (like in the code cell below), you might find some possibly surprising results, such as:\n",
    "\n",
    "- **Translations that are seemingly nonsensical or have nothing to do with the input.** This might be because the model is undertrained; you could try training for more epochs to see if the translations improve. It's also possible that you tried words or phrases that were just not well-represented in the training data.\n",
    "- **Translations that have a lot of `<unk>`s.** This might be due to the words just not being present in the model's vocabulary! Remember you can check this with the vocabularies you created, e.g. `\"friterade\" in src_vocab`. You could try increasing the vocabulary size and see if the results improve, but this will also increase training time.\n",
    "\n",
    "Finally, keep in mind that both the dataset and the model itself is quite tiny, as it's optimized for speed and demonstration purposes rather than efficiency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i miss my family .', 'close the water .', 'i love chinese cheese .']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate(['jag saknar min familj . ', 'st√§ng av vattnet .', 'jag √§lskar friterade bananer .'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7677Q0QLCutS"
   },
   "source": [
    "# Part 4: Visualising attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCzHCB4XCutS"
   },
   "source": [
    "Figure&nbsp;3 in the paper by [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473) shows some heatmaps of attention weights in selected sentences. In the last problem of this lab, we ask you to inspect attention weights for your trained translation system. We define a function `plot_attention` that visualises the attention weights. The *x*-axis corresponds to the words in the source sentence (Swedish) and the *y*-axis to the generated target sentence (English).\n",
    "\n",
    "The heatmap colours represent the **strengths of the attention weights**, with _lighter_ cells indicating a _higher_ attention value, just as in the Bahdanau et al. paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "IXHPiuamCutS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "def plot_attention(translator, sentence):\n",
    "    translation, weights = translator.translate_with_attention([sentence])[0]\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(weights, cmap='Blues_r', vmin=0., vmax=1.)\n",
    "\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(weights.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(weights.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticklabels(sentence.split(), minor=False, rotation=40)\n",
    "    ax.set_yticklabels(translation.split(), minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGCwchIdCutS"
   },
   "source": [
    "Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Hj4teeITCutT"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"377.1701pt\" height=\"307.21435pt\" viewBox=\"0 0 377.1701 307.21435\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-04-12T12:02:55.009318</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.8.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 307.21435 \n",
       "L 377.1701 307.21435 \n",
       "L 377.1701 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 35.7125 294.315912 \n",
       "L 321.4085 294.315912 \n",
       "L 321.4085 28.203912 \n",
       "L 35.7125 28.203912 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 59.5205 294.315912 \n",
       "L 59.5205 28.203912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\"/>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- jag -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(56.071756 23.11437) rotate(-40) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-6a\" d=\"M 428 4100 \n",
       "L 428 4638 \n",
       "L 991 4638 \n",
       "L 991 4100 \n",
       "L 428 4100 \n",
       "z\n",
       "M 991 -419 \n",
       "Q 991 -897 803 -1112 \n",
       "Q 616 -1328 241 -1328 \n",
       "Q 0 -1328 -156 -1300 \n",
       "L -156 -866 \n",
       "L 38 -884 \n",
       "Q 253 -884 340 -771 \n",
       "Q 428 -659 428 -334 \n",
       "L 428 3381 \n",
       "L 991 3381 \n",
       "L 991 -419 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"LiberationSans-61\" d=\"M 1294 -63 \n",
       "Q 784 -63 528 206 \n",
       "Q 272 475 272 944 \n",
       "Q 272 1469 617 1750 \n",
       "Q 963 2031 1731 2050 \n",
       "L 2491 2063 \n",
       "L 2491 2247 \n",
       "Q 2491 2659 2316 2837 \n",
       "Q 2141 3016 1766 3016 \n",
       "Q 1388 3016 1216 2887 \n",
       "Q 1044 2759 1009 2478 \n",
       "L 422 2531 \n",
       "Q 566 3444 1778 3444 \n",
       "Q 2416 3444 2737 3151 \n",
       "Q 3059 2859 3059 2306 \n",
       "L 3059 850 \n",
       "Q 3059 600 3125 473 \n",
       "Q 3191 347 3375 347 \n",
       "Q 3456 347 3559 369 \n",
       "L 3559 19 \n",
       "Q 3347 -31 3125 -31 \n",
       "Q 2813 -31 2670 133 \n",
       "Q 2528 297 2509 647 \n",
       "L 2491 647 \n",
       "Q 2275 259 1989 98 \n",
       "Q 1703 -63 1294 -63 \n",
       "z\n",
       "M 1422 359 \n",
       "Q 1731 359 1972 500 \n",
       "Q 2213 641 2352 886 \n",
       "Q 2491 1131 2491 1391 \n",
       "L 2491 1669 \n",
       "L 1875 1656 \n",
       "Q 1478 1650 1273 1575 \n",
       "Q 1069 1500 959 1344 \n",
       "Q 850 1188 850 934 \n",
       "Q 850 659 998 509 \n",
       "Q 1147 359 1422 359 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"LiberationSans-67\" d=\"M 1713 -1328 \n",
       "Q 1159 -1328 831 -1111 \n",
       "Q 503 -894 409 -494 \n",
       "L 975 -413 \n",
       "Q 1031 -647 1223 -773 \n",
       "Q 1416 -900 1728 -900 \n",
       "Q 2569 -900 2569 84 \n",
       "L 2569 628 \n",
       "L 2563 628 \n",
       "Q 2403 303 2125 139 \n",
       "Q 1847 -25 1475 -25 \n",
       "Q 853 -25 561 387 \n",
       "Q 269 800 269 1684 \n",
       "Q 269 2581 583 3007 \n",
       "Q 897 3434 1538 3434 \n",
       "Q 1897 3434 2161 3270 \n",
       "Q 2425 3106 2569 2803 \n",
       "L 2575 2803 \n",
       "Q 2575 2897 2587 3128 \n",
       "Q 2600 3359 2613 3381 \n",
       "L 3147 3381 \n",
       "Q 3128 3213 3128 2681 \n",
       "L 3128 97 \n",
       "Q 3128 -1328 1713 -1328 \n",
       "z\n",
       "M 2569 1691 \n",
       "Q 2569 2103 2456 2401 \n",
       "Q 2344 2700 2139 2858 \n",
       "Q 1934 3016 1675 3016 \n",
       "Q 1244 3016 1047 2703 \n",
       "Q 850 2391 850 1691 \n",
       "Q 850 997 1034 694 \n",
       "Q 1219 391 1666 391 \n",
       "Q 1931 391 2137 547 \n",
       "Q 2344 703 2456 995 \n",
       "Q 2569 1288 2569 1691 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-6a\"/>\n",
       "       <use xlink:href=\"#LiberationSans-61\" x=\"22.216797\"/>\n",
       "       <use xlink:href=\"#LiberationSans-67\" x=\"77.832031\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 107.1365 294.315912 \n",
       "L 107.1365 28.203912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\"/>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- √§r -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(105.393402 23.11437) rotate(-40) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-e4\" d=\"M 1294 -63 \n",
       "Q 784 -63 528 206 \n",
       "Q 272 475 272 944 \n",
       "Q 272 1469 617 1750 \n",
       "Q 963 2031 1731 2050 \n",
       "L 2491 2063 \n",
       "L 2491 2247 \n",
       "Q 2491 2659 2316 2837 \n",
       "Q 2141 3016 1766 3016 \n",
       "Q 1388 3016 1216 2887 \n",
       "Q 1044 2759 1009 2478 \n",
       "L 422 2531 \n",
       "Q 566 3444 1778 3444 \n",
       "Q 2416 3444 2737 3151 \n",
       "Q 3059 2859 3059 2306 \n",
       "L 3059 850 \n",
       "Q 3059 600 3125 473 \n",
       "Q 3191 347 3375 347 \n",
       "Q 3456 347 3559 369 \n",
       "L 3559 19 \n",
       "Q 3347 -31 3125 -31 \n",
       "Q 2813 -31 2670 133 \n",
       "Q 2528 297 2509 647 \n",
       "L 2491 647 \n",
       "Q 2275 259 1989 98 \n",
       "Q 1703 -63 1294 -63 \n",
       "z\n",
       "M 1422 359 \n",
       "Q 1731 359 1972 500 \n",
       "Q 2213 641 2352 886 \n",
       "Q 2491 1131 2491 1391 \n",
       "L 2491 1669 \n",
       "L 1875 1656 \n",
       "Q 1478 1650 1273 1575 \n",
       "Q 1069 1500 959 1344 \n",
       "Q 850 1188 850 934 \n",
       "Q 850 659 998 509 \n",
       "Q 1147 359 1422 359 \n",
       "z\n",
       "M 2108 3809 \n",
       "L 2108 4384 \n",
       "L 2617 4384 \n",
       "L 2617 3809 \n",
       "L 2108 3809 \n",
       "z\n",
       "M 877 3809 \n",
       "L 877 4384 \n",
       "L 1392 4384 \n",
       "L 1392 3809 \n",
       "L 877 3809 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"LiberationSans-72\" d=\"M 444 0 \n",
       "L 444 2594 \n",
       "Q 444 2950 425 3381 \n",
       "L 956 3381 \n",
       "Q 981 2806 981 2691 \n",
       "L 994 2691 \n",
       "Q 1128 3125 1303 3284 \n",
       "Q 1478 3444 1797 3444 \n",
       "Q 1909 3444 2025 3413 \n",
       "L 2025 2897 \n",
       "Q 1913 2928 1725 2928 \n",
       "Q 1375 2928 1190 2626 \n",
       "Q 1006 2325 1006 1763 \n",
       "L 1006 0 \n",
       "L 444 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-e4\"/>\n",
       "       <use xlink:href=\"#LiberationSans-72\" x=\"55.615234\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 154.7525 294.315912 \n",
       "L 154.7525 28.203912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\"/>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- inte -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(150.239672 23.11437) rotate(-40) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-69\" d=\"M 428 4100 \n",
       "L 428 4638 \n",
       "L 991 4638 \n",
       "L 991 4100 \n",
       "L 428 4100 \n",
       "z\n",
       "M 428 0 \n",
       "L 428 3381 \n",
       "L 991 3381 \n",
       "L 991 0 \n",
       "L 428 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"LiberationSans-6e\" d=\"M 2578 0 \n",
       "L 2578 2144 \n",
       "Q 2578 2478 2512 2662 \n",
       "Q 2447 2847 2303 2928 \n",
       "Q 2159 3009 1881 3009 \n",
       "Q 1475 3009 1240 2731 \n",
       "Q 1006 2453 1006 1959 \n",
       "L 1006 0 \n",
       "L 444 0 \n",
       "L 444 2659 \n",
       "Q 444 3250 425 3381 \n",
       "L 956 3381 \n",
       "Q 959 3366 962 3297 \n",
       "Q 966 3228 970 3139 \n",
       "Q 975 3050 981 2803 \n",
       "L 991 2803 \n",
       "Q 1184 3153 1439 3298 \n",
       "Q 1694 3444 2072 3444 \n",
       "Q 2628 3444 2886 3167 \n",
       "Q 3144 2891 3144 2253 \n",
       "L 3144 0 \n",
       "L 2578 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"LiberationSans-74\" d=\"M 1731 25 \n",
       "Q 1453 -50 1163 -50 \n",
       "Q 488 -50 488 716 \n",
       "L 488 2972 \n",
       "L 97 2972 \n",
       "L 97 3381 \n",
       "L 509 3381 \n",
       "L 675 4138 \n",
       "L 1050 4138 \n",
       "L 1050 3381 \n",
       "L 1675 3381 \n",
       "L 1675 2972 \n",
       "L 1050 2972 \n",
       "L 1050 838 \n",
       "Q 1050 594 1129 495 \n",
       "Q 1209 397 1406 397 \n",
       "Q 1519 397 1731 441 \n",
       "L 1731 25 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"LiberationSans-65\" d=\"M 863 1572 \n",
       "Q 863 991 1103 675 \n",
       "Q 1344 359 1806 359 \n",
       "Q 2172 359 2392 506 \n",
       "Q 2613 653 2691 878 \n",
       "L 3184 738 \n",
       "Q 2881 -63 1806 -63 \n",
       "Q 1056 -63 664 384 \n",
       "Q 272 831 272 1713 \n",
       "Q 272 2550 664 2997 \n",
       "Q 1056 3444 1784 3444 \n",
       "Q 3275 3444 3275 1647 \n",
       "L 3275 1572 \n",
       "L 863 1572 \n",
       "z\n",
       "M 2694 2003 \n",
       "Q 2647 2538 2422 2783 \n",
       "Q 2197 3028 1775 3028 \n",
       "Q 1366 3028 1127 2754 \n",
       "Q 888 2481 869 2003 \n",
       "L 2694 2003 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-69\"/>\n",
       "       <use xlink:href=\"#LiberationSans-6e\" x=\"22.216797\"/>\n",
       "       <use xlink:href=\"#LiberationSans-74\" x=\"77.832031\"/>\n",
       "       <use xlink:href=\"#LiberationSans-65\" x=\"105.615234\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 202.3685 294.315912 \n",
       "L 202.3685 28.203912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\"/>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- min -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(197.859263 23.11437) rotate(-40) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-6d\" d=\"M 2400 0 \n",
       "L 2400 2144 \n",
       "Q 2400 2634 2265 2821 \n",
       "Q 2131 3009 1781 3009 \n",
       "Q 1422 3009 1212 2734 \n",
       "Q 1003 2459 1003 1959 \n",
       "L 1003 0 \n",
       "L 444 0 \n",
       "L 444 2659 \n",
       "Q 444 3250 425 3381 \n",
       "L 956 3381 \n",
       "Q 959 3366 962 3297 \n",
       "Q 966 3228 970 3139 \n",
       "Q 975 3050 981 2803 \n",
       "L 991 2803 \n",
       "Q 1172 3163 1406 3303 \n",
       "Q 1641 3444 1978 3444 \n",
       "Q 2363 3444 2586 3291 \n",
       "Q 2809 3138 2897 2803 \n",
       "L 2906 2803 \n",
       "Q 3081 3144 3329 3294 \n",
       "Q 3578 3444 3931 3444 \n",
       "Q 4444 3444 4676 3166 \n",
       "Q 4909 2888 4909 2253 \n",
       "L 4909 0 \n",
       "L 4353 0 \n",
       "L 4353 2144 \n",
       "Q 4353 2634 4218 2821 \n",
       "Q 4084 3009 3734 3009 \n",
       "Q 3366 3009 3161 2736 \n",
       "Q 2956 2463 2956 1959 \n",
       "L 2956 0 \n",
       "L 2400 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-6d\"/>\n",
       "       <use xlink:href=\"#LiberationSans-69\" x=\"83.300781\"/>\n",
       "       <use xlink:href=\"#LiberationSans-6e\" x=\"105.517578\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 249.9845 294.315912 \n",
       "L 249.9845 28.203912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\"/>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- v√§n -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(245.471672 23.11437) rotate(-40) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-76\" d=\"M 1916 0 \n",
       "L 1250 0 \n",
       "L 22 3381 \n",
       "L 622 3381 \n",
       "L 1366 1181 \n",
       "Q 1406 1056 1581 441 \n",
       "L 1691 806 \n",
       "L 1813 1175 \n",
       "L 2581 3381 \n",
       "L 3178 3381 \n",
       "L 1916 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-76\"/>\n",
       "       <use xlink:href=\"#LiberationSans-e4\" x=\"50\"/>\n",
       "       <use xlink:href=\"#LiberationSans-6e\" x=\"105.615234\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 297.6005 294.315912 \n",
       "L 297.6005 28.203912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\"/>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- . -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(298.198625 23.11437) rotate(-40) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-2e\" d=\"M 584 0 \n",
       "L 584 684 \n",
       "L 1194 684 \n",
       "L 1194 0 \n",
       "L 584 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-2e\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 35.7125 47.211912 \n",
       "L 321.4085 47.211912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\"/>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- i -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.990625 50.83535) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#LiberationSans-69\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 35.7125 85.227912 \n",
       "L 321.4085 85.227912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\"/>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- ' -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(30.303125 88.85135) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-27\" d=\"M 831 3019 \n",
       "L 391 3019 \n",
       "L 325 4403 \n",
       "L 900 4403 \n",
       "L 831 3019 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-27\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 35.7125 123.243912 \n",
       "L 321.4085 123.243912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\"/>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- m -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(23.882813 126.86735) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#LiberationSans-6d\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 35.7125 161.259912 \n",
       "L 321.4085 161.259912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\"/>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- not -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(18.3125 164.88335) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-6f\" d=\"M 3291 1694 \n",
       "Q 3291 806 2900 371 \n",
       "Q 2509 -63 1766 -63 \n",
       "Q 1025 -63 647 389 \n",
       "Q 269 841 269 1694 \n",
       "Q 269 3444 1784 3444 \n",
       "Q 2559 3444 2925 3017 \n",
       "Q 3291 2591 3291 1694 \n",
       "z\n",
       "M 2700 1694 \n",
       "Q 2700 2394 2492 2711 \n",
       "Q 2284 3028 1794 3028 \n",
       "Q 1300 3028 1079 2704 \n",
       "Q 859 2381 859 1694 \n",
       "Q 859 1025 1076 689 \n",
       "Q 1294 353 1759 353 \n",
       "Q 2266 353 2483 678 \n",
       "Q 2700 1003 2700 1694 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-6e\"/>\n",
       "       <use xlink:href=\"#LiberationSans-6f\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#LiberationSans-74\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 35.7125 199.275912 \n",
       "L 321.4085 199.275912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\"/>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- my -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(18.882813 202.89935) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-79\" d=\"M 597 -1328 \n",
       "Q 366 -1328 209 -1294 \n",
       "L 209 -872 \n",
       "Q 328 -891 472 -891 \n",
       "Q 997 -891 1303 -119 \n",
       "L 1356 16 \n",
       "L 16 3381 \n",
       "L 616 3381 \n",
       "L 1328 1513 \n",
       "Q 1344 1469 1366 1408 \n",
       "Q 1388 1347 1506 1000 \n",
       "Q 1625 653 1634 613 \n",
       "L 1853 1228 \n",
       "L 2594 3381 \n",
       "L 3188 3381 \n",
       "L 1888 0 \n",
       "Q 1678 -541 1497 -805 \n",
       "Q 1316 -1069 1095 -1198 \n",
       "Q 875 -1328 597 -1328 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-6d\"/>\n",
       "       <use xlink:href=\"#LiberationSans-79\" x=\"83.300781\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 35.7125 237.291912 \n",
       "L 321.4085 237.291912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\"/>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- friend -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(7.2 240.91535) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-66\" d=\"M 1128 2972 \n",
       "L 1128 0 \n",
       "L 566 0 \n",
       "L 566 2972 \n",
       "L 91 2972 \n",
       "L 91 3381 \n",
       "L 566 3381 \n",
       "L 566 3763 \n",
       "Q 566 4225 769 4428 \n",
       "Q 972 4631 1391 4631 \n",
       "Q 1625 4631 1788 4594 \n",
       "L 1788 4166 \n",
       "Q 1647 4191 1538 4191 \n",
       "Q 1322 4191 1225 4081 \n",
       "Q 1128 3972 1128 3684 \n",
       "L 1128 3381 \n",
       "L 1788 3381 \n",
       "L 1788 2972 \n",
       "L 1128 2972 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"LiberationSans-64\" d=\"M 2566 544 \n",
       "Q 2409 219 2151 78 \n",
       "Q 1894 -63 1513 -63 \n",
       "Q 872 -63 570 368 \n",
       "Q 269 800 269 1675 \n",
       "Q 269 3444 1513 3444 \n",
       "Q 1897 3444 2153 3303 \n",
       "Q 2409 3163 2566 2856 \n",
       "L 2572 2856 \n",
       "L 2566 3234 \n",
       "L 2566 4638 \n",
       "L 3128 4638 \n",
       "L 3128 697 \n",
       "Q 3128 169 3147 0 \n",
       "L 2609 0 \n",
       "Q 2600 50 2589 231 \n",
       "Q 2578 413 2578 544 \n",
       "L 2566 544 \n",
       "z\n",
       "M 859 1694 \n",
       "Q 859 984 1046 678 \n",
       "Q 1234 372 1656 372 \n",
       "Q 2134 372 2350 703 \n",
       "Q 2566 1034 2566 1731 \n",
       "Q 2566 2403 2350 2715 \n",
       "Q 2134 3028 1663 3028 \n",
       "Q 1238 3028 1048 2714 \n",
       "Q 859 2400 859 1694 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-66\"/>\n",
       "       <use xlink:href=\"#LiberationSans-72\" x=\"27.783203\"/>\n",
       "       <use xlink:href=\"#LiberationSans-69\" x=\"61.083984\"/>\n",
       "       <use xlink:href=\"#LiberationSans-65\" x=\"83.300781\"/>\n",
       "       <use xlink:href=\"#LiberationSans-6e\" x=\"138.916016\"/>\n",
       "       <use xlink:href=\"#LiberationSans-64\" x=\"194.53125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 35.7125 275.307912 \n",
       "L 321.4085 275.307912 \n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\"/>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- . -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.434375 278.93135) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#LiberationSans-2e\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"PolyQuadMesh_1\">\n",
       "    <path d=\"M 35.7125 28.203912 \n",
       "L 35.7125 66.219912 \n",
       "L 83.3285 66.219912 \n",
       "L 83.3285 28.203912 \n",
       "L 35.7125 28.203912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #3383be\"/>\n",
       "    <path d=\"M 83.3285 28.203912 \n",
       "L 83.3285 66.219912 \n",
       "L 130.9445 66.219912 \n",
       "L 130.9445 28.203912 \n",
       "L 83.3285 28.203912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #1e6db2\"/>\n",
       "    <path d=\"M 130.9445 28.203912 \n",
       "L 130.9445 66.219912 \n",
       "L 178.5605 66.219912 \n",
       "L 178.5605 28.203912 \n",
       "L 130.9445 28.203912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #1a68ae\"/>\n",
       "    <path d=\"M 178.5605 28.203912 \n",
       "L 178.5605 66.219912 \n",
       "L 226.1765 66.219912 \n",
       "L 226.1765 28.203912 \n",
       "L 178.5605 28.203912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #084d96\"/>\n",
       "    <path d=\"M 226.1765 28.203912 \n",
       "L 226.1765 66.219912 \n",
       "L 273.7925 66.219912 \n",
       "L 273.7925 28.203912 \n",
       "L 226.1765 28.203912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083e81\"/>\n",
       "    <path d=\"M 273.7925 28.203912 \n",
       "L 273.7925 66.219912 \n",
       "L 321.4085 66.219912 \n",
       "L 321.4085 28.203912 \n",
       "L 273.7925 28.203912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083e81\"/>\n",
       "    <path d=\"M 35.7125 66.219912 \n",
       "L 35.7125 104.235912 \n",
       "L 83.3285 104.235912 \n",
       "L 83.3285 66.219912 \n",
       "L 35.7125 66.219912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083b7c\"/>\n",
       "    <path d=\"M 83.3285 66.219912 \n",
       "L 83.3285 104.235912 \n",
       "L 130.9445 104.235912 \n",
       "L 130.9445 66.219912 \n",
       "L 83.3285 66.219912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #1966ad\"/>\n",
       "    <path d=\"M 130.9445 66.219912 \n",
       "L 130.9445 104.235912 \n",
       "L 178.5605 104.235912 \n",
       "L 178.5605 66.219912 \n",
       "L 130.9445 66.219912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #6dafd7\"/>\n",
       "    <path d=\"M 178.5605 66.219912 \n",
       "L 178.5605 104.235912 \n",
       "L 226.1765 104.235912 \n",
       "L 226.1765 66.219912 \n",
       "L 178.5605 66.219912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #0e58a2\"/>\n",
       "    <path d=\"M 226.1765 66.219912 \n",
       "L 226.1765 104.235912 \n",
       "L 273.7925 104.235912 \n",
       "L 273.7925 66.219912 \n",
       "L 226.1765 66.219912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083c7d\"/>\n",
       "    <path d=\"M 273.7925 66.219912 \n",
       "L 273.7925 104.235912 \n",
       "L 321.4085 104.235912 \n",
       "L 321.4085 66.219912 \n",
       "L 273.7925 66.219912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083979\"/>\n",
       "    <path d=\"M 35.7125 104.235912 \n",
       "L 35.7125 142.251912 \n",
       "L 83.3285 142.251912 \n",
       "L 83.3285 104.235912 \n",
       "L 35.7125 104.235912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083471\"/>\n",
       "    <path d=\"M 83.3285 104.235912 \n",
       "L 83.3285 142.251912 \n",
       "L 130.9445 142.251912 \n",
       "L 130.9445 104.235912 \n",
       "L 83.3285 104.235912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #1f6eb3\"/>\n",
       "    <path d=\"M 130.9445 104.235912 \n",
       "L 130.9445 142.251912 \n",
       "L 178.5605 142.251912 \n",
       "L 178.5605 104.235912 \n",
       "L 130.9445 104.235912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #2e7ebc\"/>\n",
       "    <path d=\"M 178.5605 104.235912 \n",
       "L 178.5605 142.251912 \n",
       "L 226.1765 142.251912 \n",
       "L 226.1765 104.235912 \n",
       "L 178.5605 104.235912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #3e8ec4\"/>\n",
       "    <path d=\"M 226.1765 104.235912 \n",
       "L 226.1765 142.251912 \n",
       "L 273.7925 142.251912 \n",
       "L 273.7925 104.235912 \n",
       "L 226.1765 104.235912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083c7d\"/>\n",
       "    <path d=\"M 273.7925 104.235912 \n",
       "L 273.7925 142.251912 \n",
       "L 321.4085 142.251912 \n",
       "L 321.4085 104.235912 \n",
       "L 273.7925 104.235912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083776\"/>\n",
       "    <path d=\"M 35.7125 142.251912 \n",
       "L 35.7125 180.267912 \n",
       "L 83.3285 180.267912 \n",
       "L 83.3285 142.251912 \n",
       "L 35.7125 142.251912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08326e\"/>\n",
       "    <path d=\"M 83.3285 142.251912 \n",
       "L 83.3285 180.267912 \n",
       "L 130.9445 180.267912 \n",
       "L 130.9445 142.251912 \n",
       "L 83.3285 142.251912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08468b\"/>\n",
       "    <path d=\"M 130.9445 142.251912 \n",
       "L 130.9445 180.267912 \n",
       "L 178.5605 180.267912 \n",
       "L 178.5605 142.251912 \n",
       "L 130.9445 142.251912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #4191c6\"/>\n",
       "    <path d=\"M 178.5605 142.251912 \n",
       "L 178.5605 180.267912 \n",
       "L 226.1765 180.267912 \n",
       "L 226.1765 142.251912 \n",
       "L 178.5605 142.251912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #64a9d3\"/>\n",
       "    <path d=\"M 226.1765 142.251912 \n",
       "L 226.1765 180.267912 \n",
       "L 273.7925 180.267912 \n",
       "L 273.7925 142.251912 \n",
       "L 226.1765 142.251912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083877\"/>\n",
       "    <path d=\"M 273.7925 142.251912 \n",
       "L 273.7925 180.267912 \n",
       "L 321.4085 180.267912 \n",
       "L 321.4085 142.251912 \n",
       "L 273.7925 142.251912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083471\"/>\n",
       "    <path d=\"M 35.7125 180.267912 \n",
       "L 35.7125 218.283912 \n",
       "L 83.3285 218.283912 \n",
       "L 83.3285 180.267912 \n",
       "L 35.7125 180.267912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08316d\"/>\n",
       "    <path d=\"M 83.3285 180.267912 \n",
       "L 83.3285 218.283912 \n",
       "L 130.9445 218.283912 \n",
       "L 130.9445 180.267912 \n",
       "L 83.3285 180.267912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08326e\"/>\n",
       "    <path d=\"M 130.9445 180.267912 \n",
       "L 130.9445 218.283912 \n",
       "L 178.5605 218.283912 \n",
       "L 178.5605 180.267912 \n",
       "L 130.9445 180.267912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08468b\"/>\n",
       "    <path d=\"M 178.5605 180.267912 \n",
       "L 178.5605 218.283912 \n",
       "L 226.1765 218.283912 \n",
       "L 226.1765 180.267912 \n",
       "L 178.5605 180.267912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #d5e5f4\"/>\n",
       "    <path d=\"M 226.1765 180.267912 \n",
       "L 226.1765 218.283912 \n",
       "L 273.7925 218.283912 \n",
       "L 273.7925 180.267912 \n",
       "L 226.1765 180.267912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083979\"/>\n",
       "    <path d=\"M 273.7925 180.267912 \n",
       "L 273.7925 218.283912 \n",
       "L 321.4085 218.283912 \n",
       "L 321.4085 180.267912 \n",
       "L 273.7925 180.267912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083979\"/>\n",
       "    <path d=\"M 35.7125 218.283912 \n",
       "L 35.7125 256.299912 \n",
       "L 83.3285 256.299912 \n",
       "L 83.3285 218.283912 \n",
       "L 35.7125 218.283912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08306b\"/>\n",
       "    <path d=\"M 83.3285 218.283912 \n",
       "L 83.3285 256.299912 \n",
       "L 130.9445 256.299912 \n",
       "L 130.9445 218.283912 \n",
       "L 83.3285 218.283912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08306b\"/>\n",
       "    <path d=\"M 130.9445 218.283912 \n",
       "L 130.9445 256.299912 \n",
       "L 178.5605 256.299912 \n",
       "L 178.5605 218.283912 \n",
       "L 130.9445 218.283912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08306b\"/>\n",
       "    <path d=\"M 178.5605 218.283912 \n",
       "L 178.5605 256.299912 \n",
       "L 226.1765 256.299912 \n",
       "L 226.1765 218.283912 \n",
       "L 178.5605 218.283912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #084082\"/>\n",
       "    <path d=\"M 226.1765 218.283912 \n",
       "L 226.1765 256.299912 \n",
       "L 273.7925 256.299912 \n",
       "L 273.7925 218.283912 \n",
       "L 226.1765 218.283912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #6aaed6\"/>\n",
       "    <path d=\"M 273.7925 218.283912 \n",
       "L 273.7925 256.299912 \n",
       "L 321.4085 256.299912 \n",
       "L 321.4085 218.283912 \n",
       "L 273.7925 218.283912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #57a0ce\"/>\n",
       "    <path d=\"M 35.7125 256.299912 \n",
       "L 35.7125 294.315912 \n",
       "L 83.3285 294.315912 \n",
       "L 83.3285 256.299912 \n",
       "L 35.7125 256.299912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08306b\"/>\n",
       "    <path d=\"M 83.3285 256.299912 \n",
       "L 83.3285 294.315912 \n",
       "L 130.9445 294.315912 \n",
       "L 130.9445 256.299912 \n",
       "L 83.3285 256.299912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08306b\"/>\n",
       "    <path d=\"M 130.9445 256.299912 \n",
       "L 130.9445 294.315912 \n",
       "L 178.5605 294.315912 \n",
       "L 178.5605 256.299912 \n",
       "L 130.9445 256.299912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #08306b\"/>\n",
       "    <path d=\"M 178.5605 256.299912 \n",
       "L 178.5605 294.315912 \n",
       "L 226.1765 294.315912 \n",
       "L 226.1765 256.299912 \n",
       "L 178.5605 256.299912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083471\"/>\n",
       "    <path d=\"M 226.1765 256.299912 \n",
       "L 226.1765 294.315912 \n",
       "L 273.7925 294.315912 \n",
       "L 273.7925 256.299912 \n",
       "L 226.1765 256.299912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #083a7a\"/>\n",
       "    <path d=\"M 273.7925 256.299912 \n",
       "L 273.7925 294.315912 \n",
       "L 321.4085 294.315912 \n",
       "L 321.4085 256.299912 \n",
       "L 273.7925 256.299912 \n",
       "z\n",
       "\" clip-path=\"url(#p71276aa485)\" style=\"fill: #ebf3fb\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 35.7125 294.315912 \n",
       "L 35.7125 28.203912 \n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 321.4085 294.315912 \n",
       "L 321.4085 28.203912 \n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 35.7125 294.315912 \n",
       "L 321.4085 294.315912 \n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 35.7125 28.203912 \n",
       "L 321.4085 28.203912 \n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 339.2645 294.315912 \n",
       "L 352.5701 294.315912 \n",
       "L 352.5701 28.203912 \n",
       "L 339.2645 28.203912 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_27\"/>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(356.0701 297.93935) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-30\" d=\"M 3309 2203 \n",
       "Q 3309 1100 2920 518 \n",
       "Q 2531 -63 1772 -63 \n",
       "Q 1013 -63 631 515 \n",
       "Q 250 1094 250 2203 \n",
       "Q 250 3338 620 3903 \n",
       "Q 991 4469 1791 4469 \n",
       "Q 2569 4469 2939 3897 \n",
       "Q 3309 3325 3309 2203 \n",
       "z\n",
       "M 2738 2203 \n",
       "Q 2738 3156 2517 3584 \n",
       "Q 2297 4013 1791 4013 \n",
       "Q 1272 4013 1045 3591 \n",
       "Q 819 3169 819 2203 \n",
       "Q 819 1266 1048 831 \n",
       "Q 1278 397 1778 397 \n",
       "Q 2275 397 2506 840 \n",
       "Q 2738 1284 2738 2203 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-30\"/>\n",
       "       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#LiberationSans-30\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_28\"/>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(356.0701 244.71695) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-32\" d=\"M 322 0 \n",
       "L 322 397 \n",
       "Q 481 763 711 1042 \n",
       "Q 941 1322 1194 1548 \n",
       "Q 1447 1775 1695 1969 \n",
       "Q 1944 2163 2144 2356 \n",
       "Q 2344 2550 2467 2762 \n",
       "Q 2591 2975 2591 3244 \n",
       "Q 2591 3606 2378 3806 \n",
       "Q 2166 4006 1788 4006 \n",
       "Q 1428 4006 1195 3811 \n",
       "Q 963 3616 922 3263 \n",
       "L 347 3316 \n",
       "Q 409 3844 795 4156 \n",
       "Q 1181 4469 1788 4469 \n",
       "Q 2453 4469 2811 4155 \n",
       "Q 3169 3841 3169 3263 \n",
       "Q 3169 3006 3051 2753 \n",
       "Q 2934 2500 2703 2247 \n",
       "Q 2472 1994 1819 1463 \n",
       "Q 1459 1169 1246 933 \n",
       "Q 1034 697 941 478 \n",
       "L 3238 478 \n",
       "L 3238 0 \n",
       "L 322 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-30\"/>\n",
       "       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#LiberationSans-32\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_29\"/>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(356.0701 191.49455) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-34\" d=\"M 2753 997 \n",
       "L 2753 0 \n",
       "L 2222 0 \n",
       "L 2222 997 \n",
       "L 147 997 \n",
       "L 147 1434 \n",
       "L 2163 4403 \n",
       "L 2753 4403 \n",
       "L 2753 1441 \n",
       "L 3372 1441 \n",
       "L 3372 997 \n",
       "L 2753 997 \n",
       "z\n",
       "M 2222 3769 \n",
       "Q 2216 3750 2134 3603 \n",
       "Q 2053 3456 2013 3397 \n",
       "L 884 1734 \n",
       "L 716 1503 \n",
       "L 666 1441 \n",
       "L 2222 1441 \n",
       "L 2222 3769 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-30\"/>\n",
       "       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#LiberationSans-34\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_30\"/>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 0.6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(356.0701 138.27215) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-36\" d=\"M 3278 1441 \n",
       "Q 3278 744 2900 340 \n",
       "Q 2522 -63 1856 -63 \n",
       "Q 1113 -63 719 490 \n",
       "Q 325 1044 325 2100 \n",
       "Q 325 3244 734 3856 \n",
       "Q 1144 4469 1900 4469 \n",
       "Q 2897 4469 3156 3572 \n",
       "L 2619 3475 \n",
       "Q 2453 4013 1894 4013 \n",
       "Q 1413 4013 1148 3564 \n",
       "Q 884 3116 884 2266 \n",
       "Q 1038 2550 1316 2698 \n",
       "Q 1594 2847 1953 2847 \n",
       "Q 2563 2847 2920 2465 \n",
       "Q 3278 2084 3278 1441 \n",
       "z\n",
       "M 2706 1416 \n",
       "Q 2706 1894 2472 2153 \n",
       "Q 2238 2413 1819 2413 \n",
       "Q 1425 2413 1183 2183 \n",
       "Q 941 1953 941 1550 \n",
       "Q 941 1041 1192 716 \n",
       "Q 1444 391 1838 391 \n",
       "Q 2244 391 2475 664 \n",
       "Q 2706 938 2706 1416 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-30\"/>\n",
       "       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#LiberationSans-36\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_31\"/>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 0.8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(356.0701 85.04975) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-38\" d=\"M 3281 1228 \n",
       "Q 3281 619 2893 278 \n",
       "Q 2506 -63 1781 -63 \n",
       "Q 1075 -63 676 271 \n",
       "Q 278 606 278 1222 \n",
       "Q 278 1653 525 1947 \n",
       "Q 772 2241 1156 2303 \n",
       "L 1156 2316 \n",
       "Q 797 2400 589 2681 \n",
       "Q 381 2963 381 3341 \n",
       "Q 381 3844 757 4156 \n",
       "Q 1134 4469 1769 4469 \n",
       "Q 2419 4469 2795 4162 \n",
       "Q 3172 3856 3172 3334 \n",
       "Q 3172 2956 2962 2675 \n",
       "Q 2753 2394 2391 2322 \n",
       "L 2391 2309 \n",
       "Q 2813 2241 3047 1952 \n",
       "Q 3281 1663 3281 1228 \n",
       "z\n",
       "M 2588 3303 \n",
       "Q 2588 4050 1769 4050 \n",
       "Q 1372 4050 1164 3862 \n",
       "Q 956 3675 956 3303 \n",
       "Q 956 2925 1170 2726 \n",
       "Q 1384 2528 1775 2528 \n",
       "Q 2172 2528 2380 2711 \n",
       "Q 2588 2894 2588 3303 \n",
       "z\n",
       "M 2697 1281 \n",
       "Q 2697 1691 2453 1898 \n",
       "Q 2209 2106 1769 2106 \n",
       "Q 1341 2106 1100 1882 \n",
       "Q 859 1659 859 1269 \n",
       "Q 859 359 1788 359 \n",
       "Q 2247 359 2472 579 \n",
       "Q 2697 800 2697 1281 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-30\"/>\n",
       "       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#LiberationSans-38\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_32\"/>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 1.0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(356.0701 31.82735) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"LiberationSans-31\" d=\"M 488 0 \n",
       "L 488 478 \n",
       "L 1609 478 \n",
       "L 1609 3866 \n",
       "L 616 3156 \n",
       "L 616 3688 \n",
       "L 1656 4403 \n",
       "L 2175 4403 \n",
       "L 2175 478 \n",
       "L 3247 478 \n",
       "L 3247 0 \n",
       "L 488 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#LiberationSans-31\"/>\n",
       "       <use xlink:href=\"#LiberationSans-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#LiberationSans-30\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAABMAAAFyCAYAAAAXj+GHAAABx0lEQVR4nO2cwW0EMQwD7cU+Uk76Sv/vSw0SBrgBSBZALEWJsn1I7s/v3+dAeM99KK7DMZ1z3nMvSVaZQ4TIRL+MrlllzslCZGoH/emgD1GZK7LKHCJEpjkcrTWrzDlSZF5yNlGyw3Gl1Kwy50D7TCxT66a4Zm2N75LVzTm8bopr1tb4LlndnMPrprhmbY0NGcZVmQt4Zb4P+HsAXTOUrDKHCJmAEJnq1kDJKnOIypzDKzMnHNsaQ1TmHPBpu4M+JgMvKGaZYjfBJ5e6uSAjU0Mcjt0BY9CDTpJpZZIR5E0NcTim7ACMip8AadOKUyNlB2S4SbKZZbJNC5KF1My70dEdQH6aOM9CYhtOjdZsiD7fzNHttCALCUdtzegzrfRWJ3bT27Ta2TS3Bknm3QHoO614AlAybdOyT6sYVc4EeGXWzQWZdtC15zP0img+BWXI9E5AyFMEKjNk0Nk7ulYm7CYH8Y8LXpns3yJaU0N82BP/h5m6OSarmwsyjislHLvR56ibGzKOq+G4QFfdHHVzQdZwXJBxXF11C6S4ico8Dcc5mVgm6KZ5o5NkYplSN1PCMWPVoV/mlRnipvceIA5H7TjVzQ2ZVeY/J9EI0K+fCH8AAAAASUVORK5CYII=\" id=\"imagef9c72bdf68\" transform=\"scale(1 -1) translate(0 -266.4)\" x=\"339.12\" y=\"-27.36\" width=\"13.68\" height=\"266.4\"/>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 339.2645 294.315912 \n",
       "L 345.9173 294.315912 \n",
       "L 352.5701 294.315912 \n",
       "L 352.5701 28.203912 \n",
       "L 345.9173 28.203912 \n",
       "L 339.2645 28.203912 \n",
       "L 339.2645 294.315912 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p71276aa485\">\n",
       "   <rect x=\"35.7125\" y=\"28.203912\" width=\"285.696\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_attention(translator, 'jag √§r inte min v√§n . ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§î Task 7: Use these heatmaps to inspect the attention patterns for selected Swedish sentences\n",
    "\n",
    "Try to find sentences for which the model produces reasonably good English translations. If you don't speak Swedish, use sentences from the validation data. It might be interesting to look at examples where the Swedish and the English word order differ substantially.\n",
    "\n",
    "Based on your exploration, **answer the following questions:**\n",
    "\n",
    "- What sentences did you try out? What patterns did you spot? Include example heatmaps in your notebook.\n",
    "\n",
    "We used the Swedish sentence: \"jag saknar min familj\" (which means \"I miss my family\").\n",
    "At the beginning, the model gave strange translations like:\n",
    "\n",
    "‚Äúi hate my car‚Äù\n",
    "\n",
    "‚Äúi love my coat‚Äù\n",
    "\n",
    "But after more training, it finally gave the correct one:\n",
    "\n",
    "‚Äúi miss my family‚Äù\n",
    "\n",
    "From the attention heatmap, several patterns are visible:\n",
    "Strong alignments between source and target words appear as brighter squares (closer to white):\n",
    "\n",
    "\"min\" aligns strongly with \"my\"\n",
    "\n",
    "\"v√§n\" aligns well with \"friend\"\n",
    "\n",
    "Punctuation match is also entirely accurate, the period at the end aligns well across languages. The matrix is mostly diagonal, which is common for sentence-level translation between languages with similar word order. This diagonal attention suggests good correspondence. Yes, the results are generally what we expected. The attention model appears to be learning token alignments effectively, especially in later epochs.\n",
    "\n",
    "- Based on what you know about attention, did you expect your results? Was there anything surprising in them?\n",
    "\n",
    "Yes, the results mostly matched what we expected. The attention model seems to be learning how to align tokens well, especially during the later stages of training.\n",
    "\n",
    "Early on, the translations didn‚Äôt make much sense (like ‚ÄúI love my hair‚Äù), but the attention maps still showed meaningful word connections. This means the model had a sense of where to focus, even before it fully understood how to translate properly.\n",
    "\n",
    "We also noticed some word repetition in the middle of training (like ‚ÄúI already already already‚Ä¶‚Äù), which is a common issue in these types of models before they are fully trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü•≥ Congratulations on finishing this lab! ü•≥**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-L5-WithSolutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
